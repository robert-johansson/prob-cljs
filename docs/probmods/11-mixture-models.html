<!DOCTYPE html>
<html>
<head>
  <title>ProbMods: Mixture Models</title>
  <meta charset="UTF-8">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js" onload="renderMathInElement(document.body,{delimiters:[{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}]})"></script>
  <script src="../js/editor.js"></script>
  <script src="../js/physics.js"></script>
  <script>ProbPhysics.setup();</script>
  <script src="https://cdn.jsdelivr.net/npm/scittle@0.7.28/dist/scittle.js" crossorigin="anonymous"></script>
</head>
<body>
<div id="chapter-wrapper">
  <div id="header">
    <div id="logotype"><a href="index.html">Probabilistic Models of Cognition</a></div>
    <ul id="nav">
      <li><a href="10-occams-razor.html">&larr; Prev</a></li>
      <li><a href="index.html">Index</a></li>
      <li><a href="12-non-parametric-models.html">Next &rarr;</a></li>
    </ul>
  </div>
  <div id="chapter">


<h1 id="chapter-title">11. Mixture Models</h1>

<div class="toc">
<div class="name">Contents:</div>
<ul>
<li><a href="#learning-categories">Learning Categories</a></li>
<li><a href="#topic-models">Example: Topic Models</a></li>
<li><a href="#categorical-perception">Example: Categorical Perception of Speech Sounds</a></li>
<li><a href="#unknown-numbers">Unknown Numbers of Categories</a></li>
</ul>
</div>


<p>In the chapter on <a href="09-hierarchical-models.html">Hierarchical Models</a>, we saw the power of probabilistic inference in learning about the latent structure underlying different kinds of observations: the mixture of colors in different bags of marbles, or the prototypical features of categories of animals. In that discussion we always assumed that we knew what kind each observation belonged to &mdash; the bag that each marble came from, or the subordinate, basic, and superordinate category of each object. Knowing this allowed us to pool the information from each observation for the appropriate latent variables. What if we don&rsquo;t know <em>a priori</em> how to divide up our observations? In this chapter we explore the problem of simultaneously discovering kinds and their properties &mdash; this can be done using <em>mixture models</em>.</p>


<h1 id="learning-categories"><a href="#learning-categories">Learning Categories</a></h1>

<p>Imagine a child who enters the world and begins to see objects. She can&rsquo;t begin by learning the typical features of cats or mice, because she doesn&rsquo;t yet know that there are such kinds of objects as cats and mice. Yet she may quickly notice that some of the objects all tend to purr and have claws, while other objects are small and run fast &mdash; she can <em>cluster</em> the objects together on the basis of common features and thus form categories (such as cats and mice), whose typical features she can then learn.</p>

<p>To formalize this learning problem, we begin by adapting the bags-of-marbles examples from the <a href="09-hierarchical-models.html">Hierarchical Models</a> chapter. However, we now assume that the bag that each marble is drawn from is <em>unobserved</em> and must be inferred.</p>

<pre><code>(def colors ['blue 'green 'red])

(def samples
  (mh-query
   200 100
   (let [phi (dirichlet [1 1 1])
         alpha 0.1
         prototype (mapv (fn [w] (* alpha w)) phi)
         bag->prototype (mem (fn [bag] (dirichlet prototype)))
         obs->bag (mem (fn [obs-name]
                         (uniform-draw ['bag1 'bag2 'bag3])))
         draw-marble (mem (fn [obs-name]
                            (multinomial colors (bag->prototype (obs->bag obs-name)))))]
     (condition (and
                  (= 'red (draw-marble 'obs1))
                  (= 'red (draw-marble 'obs2))
                  (= 'blue (draw-marble 'obs3))
                  (= 'blue (draw-marble 'obs4))
                  (= 'red (draw-marble 'obs5))
                  (= 'blue (draw-marble 'obs6))))
     [(= (obs->bag 'obs1) (obs->bag 'obs2))
      (= (obs->bag 'obs1) (obs->bag 'obs3))])))

(hist (map first samples) "obs1 and obs2 same category?")
(hist (map second samples) "obs1 and obs3 same category?")</code></pre>

<p>We see that it is likely that <code>obs1</code> and <code>obs2</code> came from the same bag, but quite unlikely that <code>obs3</code> did. Why? Notice that we have set <code>alpha</code> small, indicating a belief that the marbles in a bag will tend to all be the same color. How do the results change if you make <code>alpha</code> larger? Why? Note that we have queried on whether observed marbles came out of the same bag, instead of directly querying on the bag number that an observation came from. This is because the bag number by itself is meaningless &mdash; it is only useful in its role of determining which objects have similar properties. Formally, the model we have defined above is symmetric in the bag labels (if you permute all the labels you get a new state with the same probability).</p>

<p>Instead of assuming that a marble is equally likely to come from each bag, we could instead learn a distribution over bags where each bag has a different probability. This is called a <em>mixture distribution</em> over the bags:</p>

<pre><code>(def colors ['blue 'green 'red])

(def samples
  (mh-query
   200 100
   (let [phi (dirichlet [1 1 1])
         alpha 0.1
         prototype (mapv (fn [w] (* alpha w)) phi)
         bag->prototype (mem (fn [bag] (dirichlet prototype)))
         bag-mixture (dirichlet [1 1 1])
         obs->bag (mem (fn [obs-name]
                         (multinomial ['bag1 'bag2 'bag3] bag-mixture)))
         draw-marble (mem (fn [obs-name]
                            (multinomial colors (bag->prototype (obs->bag obs-name)))))]
     (condition (and
                  (= 'red (draw-marble 'obs1))
                  (= 'red (draw-marble 'obs2))
                  (= 'blue (draw-marble 'obs3))
                  (= 'blue (draw-marble 'obs4))
                  (= 'red (draw-marble 'obs5))
                  (= 'blue (draw-marble 'obs6))))
     [(= (obs->bag 'obs1) (obs->bag 'obs2))
      (= (obs->bag 'obs1) (obs->bag 'obs3))])))

(hist (map first samples) "obs1 and obs2 same category?")
(hist (map second samples) "obs1 and obs3 same category?")</code></pre>

<p>Models of this kind are called <strong>mixture models</strong> because the observations are a &ldquo;mixture&rdquo; of several categories. Mixture models are widely used in modern probabilistic modeling because they describe how to learn the unobservable categories which underlie observable properties in the world.</p>

<p>The observation distribution associated with each mixture <em>component</em> (i.e., kind or category) can be any distribution we like. For example, here is a mixture model with <em>Gaussian</em> components:</p>

<pre><code>(def samples
  (mh-query
   200 100
   (let [bag-mixture (dirichlet [1 1])
         obs->cat (mem (fn [obs-name]
                         (multinomial ['bag1 'bag2] bag-mixture)))
         cat->mean (mem (fn [cat] [(gaussian 0.0 1.0) (gaussian 0.0 1.0)]))
         observe-point (fn [obs-name value]
                         (doall (map (fn [x-mean y]
                                       (observe (gaussian-dist x-mean 0.1) y))
                                     (cat->mean (obs->cat obs-name))
                                     value)))]
     ;; one cluster of points in the top right quadrant
     (observe-point 'a1 [0.5 0.5])
     (observe-point 'a2 [0.6 0.5])
     (observe-point 'a3 [0.5 0.4])
     (observe-point 'a4 [0.55 0.55])
     (observe-point 'a5 [0.45 0.45])
     (observe-point 'a6 [0.5 0.5])
     (observe-point 'a7 [0.7 0.6])
     ;; another cluster in the lower left quadrant
     (observe-point 'b1 [-0.5 -0.5])
     (observe-point 'b2 [-0.7 -0.4])
     (observe-point 'b3 [-0.5 -0.6])
     (observe-point 'b4 [-0.55 -0.55])
     (observe-point 'b5 [-0.5 -0.45])
     (observe-point 'b6 [-0.6 -0.5])
     (observe-point 'b7 [-0.6 -0.4])
     (mapv cat->mean ['bag1 'bag2]))))

(scatter (map first samples) "bag 1 mean")
(scatter (map second samples) "bag 2 mean")</code></pre>


<h2 id="topic-models"><a href="#topic-models">Example: Topic Models</a></h2>

<p>One very popular class of mixture-based approaches are <em>topic models</em>, which are used for document classification, clustering, and retrieval. The simplest kind of topic models make the assumption that documents can be represented as <em>bags of words</em> &mdash; unordered collections of the words that the document contains. In topic models, each document is associated with a mixture over <em>topics</em>, each of which is itself a distribution over words.</p>

<p>One popular kind of bag-of-words topic model is known as <em>Latent Dirichlet Allocation</em> (LDA) (Blei, Ng, &amp; Jordan, 2003). The generative process for this model can be described as follows. For each document, mixture weights over a set of \(K\) topics are drawn from a Dirichlet prior. Then \(N\) topics are sampled for the document &mdash; one for each word. Each topic itself is associated with a distribution over words, and this distribution is drawn from a Dirichlet prior. For each of the \(N\) topics drawn for the document, a word is sampled from the corresponding multinomial distribution. This is shown in the code below.</p>

<pre><code>(def vocabulary ['DNA 'evolution 'parsing 'phonology])
(def topics ['topic1 'topic2])
(def doc-length 10)

(def samples
  (mh-query
   200 100
   (let [document->mixture-params (mem (fn [doc-id]
                                         (dirichlet (vec (repeat (count topics) 1.0)))))
         topic->mixture-params (mem (fn [topic]
                                      (dirichlet (vec (repeat (count vocabulary) 0.1)))))
         document->topics (mem (fn [doc-id]
                                 (vec (repeatedly doc-length
                                        (fn [] (multinomial topics (document->mixture-params doc-id)))))))
         observe-document (fn [doc-id words]
                            (let [doc-topics (document->topics doc-id)
                                  topic-mixtures (map topic->mixture-params doc-topics)]
                              (doall (map (fn [topic-mixture word]
                                            (condition (= (multinomial vocabulary topic-mixture) word)))
                                          topic-mixtures words))))]
     (observe-document 'a1 ['DNA 'evolution 'DNA 'evolution 'DNA 'evolution 'DNA 'evolution 'DNA 'evolution])
     (observe-document 'a2 ['DNA 'evolution 'DNA 'evolution 'DNA 'evolution 'DNA 'evolution 'DNA 'evolution])
     (observe-document 'a3 ['DNA 'evolution 'DNA 'evolution 'DNA 'evolution 'DNA 'evolution 'DNA 'evolution])
     (observe-document 'b1 ['parsing 'phonology 'parsing 'phonology 'parsing 'phonology 'parsing 'phonology 'parsing 'phonology])
     (observe-document 'b2 ['parsing 'phonology 'parsing 'phonology 'parsing 'phonology 'parsing 'phonology 'parsing 'phonology])
     (observe-document 'b3 ['parsing 'phonology 'parsing 'phonology 'parsing 'phonology 'parsing 'phonology 'parsing 'phonology])
     [(topic->mixture-params 'topic1) (topic->mixture-params 'topic2)])))

(defn mat-row-mean [m]
  (let [n (count m)]
    (mapv #(/ % n) (reduce (fn [a b] (mapv + a b)) m))))

(barplot [vocabulary (mat-row-mean (map first samples))] "Distribution over words for Topic 1")
(barplot [vocabulary (mat-row-mean (map second samples))] "Distribution over words for Topic 2")</code></pre>

<p>In this simple example, there are two topics <code>topic1</code> and <code>topic2</code>, and four words. These words are deliberately chosen to represent one of two possible subjects that a document can be about: One can be thought of as &ldquo;biology&rdquo; (i.e., <code>DNA</code> and <code>evolution</code>), and the other can be thought of as &ldquo;linguistics&rdquo; (i.e., <code>parsing</code> and <code>phonology</code>).</p>

<p>The documents consist of lists of individual words from one or the other topic. Based on the co-occurrence of words within individual documents, the model is able to learn that one of the topics should put high probability on the biological words and the other topic should put high probability on the linguistic words. It is able to learn this because different kinds of documents represent stable mixtures of different kinds of topics which in turn represent stable distributions over words.</p>


<h2 id="categorical-perception"><a href="#categorical-perception">Example: Categorical Perception of Speech Sounds</a></h2>

<p>(This example is adapted from: Feldman, N. H., Griffiths, T. L., and Morgan, J. L. (2009). The influence of categories on perception: Explaining the perceptual magnet effect as optimal statistical inference. <em>Psychological Review</em>, 116(4):752&ndash;782.)</p>

<p>Human perception is often skewed by our expectations. A common example of this is called <em>categorical perception</em> &mdash; when we perceive objects as being more similar to the category prototype than they really are. In phonology this has been particularly important and is called the perceptual magnet effect: Hearers regularize a speech sound into the category that they think it corresponds to. Of course this category isn&rsquo;t known a priori, so a hearer must be doing a simultaneous inference of what category the speech sound corresponded to, and what the sound must have been. In the below code we model this as a mixture model over the latent categories of sounds, combined with a noisy observation process.</p>

<pre><code>(def prototype-1 8.0)
(def prototype-2 10.0)

(defn compute-pair-distance [stimulus-1 stimulus-2]
  (mean
   (mh-query 500 5
    (let [sample-target (fn [] (if (flip)
                                 (gaussian prototype-1 0.5)
                                 (gaussian prototype-2 0.5)))
          target-1 (sample-target)
          target-2 (sample-target)]
      (observe (gaussian-dist target-1 0.2) stimulus-1)
      (observe (gaussian-dist target-2 0.2) stimulus-2)
      (abs (- target-1 target-2))))))

(defn compute-perceptual-pairs [lst]
  (mapv compute-pair-distance lst (rest lst)))

(defn compute-stimuli-pairs [lst]
  (mapv (fn [a b] (abs (- a b))) lst (rest lst)))

(def stimuli (mapv #(+ prototype-1 (* % 0.1)) (range 21)))

(def stimulus-distances (compute-stimuli-pairs stimuli))
(def perceptual-distances (compute-perceptual-pairs stimuli))

(scatter (mapv vector (range (count stimulus-distances))
                      stimulus-distances)
         "Stimulus Distances")

(scatter (mapv vector (range (count perceptual-distances))
                      perceptual-distances)
         "Perceptual Distances")</code></pre>

<p>Notice that the perceived distances between input sounds are skewed relative to the actual acoustic distances &mdash; that is they are attracted towards the category centers.</p>


<h1 id="unknown-numbers"><a href="#unknown-numbers">Unknown Numbers of Categories</a></h1>

<p>The models above describe how a learner can simultaneously learn which category each object belongs to, the typical properties of objects in that category, and even global parameters about kinds of objects in general. However, they suffer from a serious flaw: the number of categories was fixed. This is as if a learner, after finding out there are cats, dogs, and mice, must force an elephant into one of these categories, for want of more categories to work with.</p>

<p>The simplest way to address this problem, which we call <em>unbounded</em> models, is to simply place uncertainty on the number of categories in the form of a hierarchical prior. Let&rsquo;s warm up with a simple example of this: inferring whether one or two coins were responsible for a set of outcomes (i.e. imagine a friend is shouting each outcome from the next room &mdash; &ldquo;heads, heads, tails...&rdquo; &mdash; is she using a fair coin, or two biased coins?).</p>

<pre><code>(def actual-obs [true true true true false false false false])

(def samples
  (mh-query
   200 100
   (let [coins (if (flip) ['c1] ['c1 'c2])
         coin->weight (mem (fn [c] (uniform 0 1)))]
     (doseq [v actual-obs]
       (condition (= (flip (coin->weight (uniform-draw coins))) v)))
     (count coins))))

(hist samples "number of coins")</code></pre>

<p>How does the inferred number of coins change as the amount of data grows? Why?</p>

<p>We could extend this model by allowing it to infer that there are more than two coins. However, no evidence requires us to posit three or more coins (we can always explain the data as &ldquo;a heads coin and a tails coin&rdquo;). Instead, let us apply the same idea to the marbles examples above:</p>

<pre><code>(def colors ['blue 'green 'red])

(def samples
  (mh-query
   200 100
   (let [phi (dirichlet [1 1 1])
         alpha 0.1
         prototype (mapv (fn [w] (* alpha w)) phi)
         bag->prototype (mem (fn [bag] (dirichlet prototype)))
         num-bags (+ 1 (poisson 1.0))
         bags (vec (repeatedly num-bags gensym))]
     (doseq [m ['red 'red 'blue 'blue 'red 'blue]]
       (condition (= (multinomial colors (bag->prototype (uniform-draw bags))) m)))
     num-bags)))

(hist samples "how many bags?")</code></pre>

<p>Vary the amount of evidence and see how the inferred number of bags changes.</p>

<p>For the prior on <code>num-bags</code> we used the <a href="http://en.wikipedia.org/wiki/Poisson_distribution"><em>Poisson distribution</em></a> which is a distribution on non-negative integers. It is convenient, though implies strong prior knowledge (perhaps too strong for this example). We have created <code>gensym</code> using <code>make-gensym</code>; a <code>gensym</code> function returns a fresh symbol every time it is called. It can be used to generate an unbounded set of labels for things like classes, categories and mixture components. Each evaluation of <code>gensym</code> results in a unique (although cryptic) symbol:</p>

<pre><code>[(gensym) (gensym) (gensym)]</code></pre>

<p>Importantly, these symbols can be used as identifiers, because two different calls to gensym will never be equal:</p>

<pre><code>(= (gensym) (gensym))</code></pre>

<p>For clarity, you can use <code>make-gensym</code> to create a gensym function with a custom prefix:</p>

<pre><code>(def my-gensym (make-gensym "foo"))
[(my-gensym) (my-gensym) (my-gensym)]</code></pre>

<p>Unbounded models give a straightforward way to represent uncertainty over the number of categories in the world. However, inference in these models often presents difficulties. In the <a href="12-non-parametric-models.html">next chapter</a> we describe another method for allowing an unknown number of things: In an unbounded model, there are a finite number of categories whose number is drawn from an unbounded prior distribution, such as the Poisson prior that we just examined. In an &ldquo;infinite model&rdquo; we construct distributions assuming a truly infinite number of objects.</p>


<h1 id="references"><a href="#references">References</a></h1>

<p>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet allocation. <em>Journal of Machine Learning Research</em>, <em>3</em>, 993&ndash;1022.</p>
<p>Feldman, N. H., Griffiths, T. L., &amp; Morgan, J. L. (2009). The influence of categories on perception: Explaining the perceptual magnet effect as optimal statistical inference. <em>Psychological Review</em>, <em>116</em>(4), 752&ndash;782.</p>


  </div>
  <div class="chapter-nav">
    <a href="10-occams-razor.html">&larr; Chapter 10: Occam&rsquo;s Razor</a>
    <a href="12-non-parametric-models.html">Chapter 12: Non-parametric Models &rarr;</a>
  </div>
</div>

<!-- Interactive runner: loads prob-cljs via fetch + eval_string -->
<script>
var _scittleReady = false;
var _scittleLoading = null;

function ensureScittleImports() {
  if (_scittleReady) return Promise.resolve();
  if (_scittleLoading) return _scittleLoading;

  var files = [
    '../prob/math.cljs',
    '../prob/erp.cljs',
    '../prob/dist.cljs',
    '../prob/inference.cljs',
    '../prob/builtins.cljs',
    '../prob/core.cljs',
    'viz.cljs'
  ];

  _scittleLoading = Promise.all(files.map(function(f) {
    return fetch(f).then(function(r) {
      if (!r.ok) throw new Error('Failed to load ' + f + ': ' + r.status);
      return r.text();
    });
  })).then(function(sources) {
    sources.forEach(function(src) { scittle.core.eval_string(src); });

    scittle.core.eval_string(
      "(ns prob.macros)" +
      "(defmacro rejection-query [& body] `(prob.core/rejection-query-fn (fn [] ~@body)))" +
      "(defmacro mh-query [n lag & body] `(prob.core/mh-query-fn ~n ~lag (fn [] ~@body)))" +
      "(defmacro enumeration-query [& body] `(prob.core/enumeration-query-fn (fn [] ~@body)))"
    );

    scittle.core.eval_string(
      "(require '[prob.core :refer [flip gaussian uniform uniform-draw random-integer multinomial" +
      "                              sample-discrete beta gamma dirichlet exponential" +
      "                              binomial poisson categorical" +
      "                              condition factor observe rejection-query-fn mh-query-fn" +
      "                              enumeration-query-fn mem mean variance sum prod" +
      "                              sample* observe* dist? enumerate*]])" +
      "(require '[prob.dist :refer [observe* sample* dist? enumerate*" +
      "                              gaussian-dist bernoulli-dist uniform-dist beta-dist gamma-dist" +
      "                              exponential-dist dirichlet-dist uniform-draw-dist" +
      "                              random-integer-dist multinomial-dist sample-discrete-dist" +
      "                              binomial-dist poisson-dist categorical-dist]])" +
      "(require '[prob.builtins :refer [expt member pair null? equal? make-list length" +
      "                                  string-append abs sample fold iota make-gensym gensym]])" +
      "(require '[prob.macros :refer [rejection-query mh-query enumeration-query]])" +
      "(require '[prob.viz :refer [hist density scatter barplot lineplot display run-physics animate-physics]])"
    );

    _scittleReady = true;
  });

  return _scittleLoading;
}

function runCode(code, output) {
  output.innerHTML = '';
  window.__currentOutput = output;
  window.__appendToOutput = function(el) { window.__currentOutput.appendChild(el); };
  window.__appendTextToOutput = function(text) {
    var span = document.createElement('span');
    span.textContent = text + '\n';
    window.__currentOutput.appendChild(span);
  };
  ensureScittleImports().then(function() {
    window.__currentOutput = output;
    try {
      var result = scittle.core.eval_string(code);
      if (result != null) {
        var span = document.createElement('span');
        span.textContent = '' + result;
        output.appendChild(span);
      }
    } catch(e) {
      var span = document.createElement('span');
      span.className = 'error';
      span.textContent = 'Error: ' + e.message;
      output.appendChild(span);
    }
  }).catch(function(e) {
    var span = document.createElement('span');
    span.className = 'error';
    span.textContent = 'Load error: ' + e.message;
    output.appendChild(span);
  });
}

document.querySelectorAll('#chapter pre:not(.norun)').forEach(function(pre) {
  var codeEl = pre.querySelector('code');
  if (!codeEl) return;
  var code = codeEl.textContent.trim();

  var container = document.createElement('div');
  container.className = 'code-example';

  var editorDiv = document.createElement('div');

  var toolbar = document.createElement('div');
  toolbar.className = 'toolbar';

  var btn = document.createElement('button');
  btn.textContent = 'Run';

  var output = document.createElement('div');
  output.className = 'output';

  toolbar.appendChild(btn);
  container.appendChild(editorDiv);
  container.appendChild(toolbar);
  container.appendChild(output);
  pre.replaceWith(container);

  var editor = ProbEditor.createEditor(editorDiv, code, {
    onEval: function(code) { runCode(code, output); }
  });

  btn.addEventListener('click', function() {
    runCode(editor.getCode(), output);
  });
});
</script>
</body>
</html>
